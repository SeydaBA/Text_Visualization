
```{r}
# set options
#options(stringsAsFactors = F)         # no automatic data transformation
#options("scipen" = 100, "digits" = 4) # supress math annotation
# install libraries
#install.packages(c("tm", "topicmodels", "reshape2", "ggplot2", "wordcloud", "pals", "igraph","qdap"))
#install.packages("circlize", repos='http://cran.us.r-project.org')
```


```{r}
library(tm)
library(topicmodels)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(qdap)
library(circlize)
library(dendextend) 
library(tidytext)
library(SnowballC) 
library(stringi)
library(topicmodels) # for LDA topic modelling 
library(dplyr)
```


```{r}
library(readxl)
AJ <- read_excel("C:/Users/LENOVO/Desktop/AJ.xlsx")
tweets=data.frame(doc_id=seq(1:nrow(AJ)),text=AJ$text) 
```

```{r}
tweets$text<-gsub("\\b's\\b","",as.character(tweets$text))
```


```{r}
#create a function to change case to lower
tryTolower <- function( x){ 
  y = NA 
  try_error = tryCatch( tolower( x), error = function( e) e) 
if (! inherits( try_error, 'error')) 
  y = tolower( x) 
  return( y) }

```

```{r}
custom.stopwords = c( stopwords("english"), "lol", "smh","site", "delta","said", "amp","haftars","also","video","play","keep","reading","will")
```


```{r}
#now set up a function to clean up a corpus


clean.corpus <- function( corpus){ 
  corpus <- tm_map( corpus, content_transformer( tryTolower)) 
  corpus = tm_map( corpus, removeWords, custom.stopwords) 
  corpus = tm_map( corpus, removeWords, c("haftar's","libya's","libyan","since","will","april","said","haftar'S")) 
  corpus = tm_map( corpus, removePunctuation) 
  corpus = tm_map( corpus, stripWhitespace) 
  corpus = tm_map( corpus, removeNumbers) 
  return( corpus) 
  } 
```


```{r}
corpus <- VCorpus( DataframeSource( tweets)) 
corpus <- clean.corpus( corpus) 
corpus <- tm_map(corpus, content_transformer(function(x) 
    gsub(x, pattern = "\\b's\\b", replacement = " ")))
tdm <- TermDocumentMatrix( corpus, control = list( weighting = weightTf))
tdm.tweets.m <- as.matrix( tdm) 
term.freq <- rowSums( tdm.tweets.m) 
freq.df <- data.frame( word = names( term.freq), frequency = term.freq) 
freq.df <- freq.df[ order( freq.df[, 2], decreasing = T),]
```



```{r}
# plot the terms by frequency
freq.df$ word <- factor( freq.df $ word, 
                          levels = unique( as.character( freq.df $ word))) 
ggplot( freq.df[ 1: 20,], aes( x = word, y = frequency)) + 
  geom_bar( stat ="identity", fill ='royalblue3') + 
  coord_flip() + 
  geom_text( aes( label = frequency), colour ="black", hjust = 1.25, size = 5.0)
```



```{r}
#find associations
associations = findAssocs( tdm, 'haftar', 0.40) 
associations = as.data.frame( associations) 
associations $ terms = row.names( associations)
associations $ terms <- factor( associations $ terms, levels = associations $ terms)
```

```{r}
#plot the associations
ggplot( associations, aes( y = terms)) + 
  geom_point( aes( x = haftar), data = associations, size = 5) + 
  geom_text( aes( x = haftar, label = haftar), colour ="darkred", hjust = -0.25, size = 3) + 
  theme( text = element_text( size = 10), axis.title.y = element_blank())
```

```{r}
refund <- tweets[ grep("libya", tweets$text, ignore.case = T), ]
```


```{r}
#refund.corpus <âVCorpus( DataframeSource( refund[ 1: 3,]), readerControl = list( reader = refund.reader)) 
#burada refund'u filter etmemiz lazım

refund.corpus <-VCorpus( DataframeSource( refund[177:177,]))
refund.corpus <- clean.corpus( refund.corpus) 
refund.tdm <- TermDocumentMatrix( refund.corpus, control = list( weighting = weightTf))

library( igraph) 
refund.m <- as.matrix( refund.tdm)
refund.adj = refund.m %*% t( refund.m) 
refund.adj = graph.adjacency( refund.adj, weighted = TRUE, mode ="undirected", diag = T) 
refund.adj = simplify( refund.adj)
```


```{r}
plot.igraph( refund.adj, vertex.shape ="none", 
             vertex.label.font = 1,
             vertex.label.color ="royalblue3", 
             vertex.label.cex = .7, 
             edge.color ="gray85") 
              title( main ='Libya Word Network')
```


```{r}
#dont run
#word_network_plot( refund $ text[ 177: 177]) 
        #title( main ='Libya Word Network')

#word_associate( tweets $ text, match.string = c('libya'), 
                #stopwords = Top200Words, 
                #network.plot = T, 
                #cloud.colors = c('gray85','darkred')) 
```


```{r}

tdm2 <- removeSparseTerms( tdm, sparse = 0.45)
hc <- hclust( dist( tdm2, method ="euclidean"), method ="complete")
plot( hc, yaxt ='n', main ='Al Jazeera - Dendrogram')
```

```{r}
dend.change <- function( n) { if (is.leaf( n)) {
  a <- attributes( n) 
  labCol <- labelColors[ clusMember[ which( names( clusMember) == a $ label)]] 
  attr( n, "nodePar") <- c( a $ nodePar, lab.col = labCol) 
} 
  n 
  }
hcd = as.dendrogram( hc) 
clusMember =cutree( hc, 4) 
labelColors = c('orange', 'darkred', 'black', '#bada55') 
clusDendro = dendrapply( hcd, dend.change) 
plot( clusDendro, main = "Triangle Dendrogram", type = "triangle", yaxt ='n')
```


```{r}
hcd <- color_labels( hcd, 4, col = c('darkblue','orange', "black", 'darkred')) 
hcd <- color_branches( hcd, 4, col = c('darkblue','orange', "black", 'darkred')) 
circlize_dendrogram( hcd, labels_track_height = 0.5, dend_track_height = 0.4)
```

```{r}
library( tm) 
library( wordcloud) 
tryTolower <- function( x){ 
  y = NA 
  try_error = tryCatch( tolower( x), error = function( e) e) 
if (!inherits( try_error, 'error')) 
  y = tolower( x) 
return( y) 
} 
custom.stopwords <- c( stopwords('english'), 'sorry', 'amp', 'delta', 'amazon','video',"haftar's","play","keep","reading","also","since","last","libyan","said") 
clean.vec <- function( text.vec){ text.vec <- tryTolower( text.vec) 
  text.vec <- removeWords( text.vec, custom.stopwords) 
  text.vec <- removePunctuation( text.vec) 
  text.vec <- stripWhitespace( text.vec) 
  text.vec <- removeNumbers( text.vec) 
  return( text.vec) 
  }
```

```{r}
TR <- read_excel("C:/Users/LENOVO/Desktop/TR.xlsx")
TR <- clean.vec( TR $ text)
AJ <- read_excel("C:/Users/LENOVO/Desktop/AJ.xlsx")
AJ <- clean.vec( AJ $ text)

```

```{r}
TR <- paste( TR, collapse = " ") 
AJ <- paste( AJ, collapse = " ") 
all <- c( TR, AJ) 
corpus <- VCorpus( VectorSource( all))
```

```{r}
tdm = TermDocumentMatrix( corpus) 
tdm.m = as.matrix( tdm) 
#name the columns
colnames( tdm.m)<- c("TRT", "AJ")
tdm.m[3480:3490,]
```

```{r}
display.brewer.all()
```
```{r}
#pick purples can be any color
pal <- brewer.pal( 8, "Purples")
#use the darker colors
pal <- pal[-( 1: 4)]
#generate the commonality cloud
commonality.cloud( tdm.m, max.words = 200, random.order = FALSE, colors = pal)
```

```{r}
comparison.cloud( tdm.m, max.words = 200, random.order = FALSE, title.size = 1.0, rot.per = 0.35,
                  colors = brewer.pal( 8,"Dark2"))
```

```{r}
library( plotrix) 
common.words <- subset( tdm.m, tdm.m[, 1] > 0 & tdm.m[, 2] > 0)
tail( common.words)
```

```{r}
#calculate the differences between the two columns of common words

difference <- abs( common.words[, 1] - common.words[, 2])

#combine the differences with the common words 
common.words <- cbind( common.words, difference) 
#sort by the difference column in decreasing order
common.words <- common.words[ order( common.words[, 3], decreasing = TRUE), ]

#select the top 25 words and create a data frame
top25.df <- data.frame( x = common.words[ 1: 25, 1], 
                         y = common.words[ 1: 25, 2], 
                         labels = rownames(common.words[ 1: 25, ]))
```

```{r}
pyramid.plot(top25.df$x, top25.df$y, 
              labels = top25.df$labels, 
#change gap to show longer words
                          gap = 250, labelcex = .8,
              top.labels = c("TRT World", "Words", "Al Jazeera"), 
              main = "Words in Common", 
              laxlab = NULL, raxlab = NULL, unit = NULL)
```
